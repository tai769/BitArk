# 架构决策记录：为什么 Slave 必须维护自己的 LSN 进度快照？

> **📅 日期**: 2026-01-30
> **🏷️ 标签**: `Architecture` `Distributed System` `Replication` `Reliability`
> **📌 核心结论**: 在分布式存储架构中，**Master 的内存状态是不可信的，网络的传输也是不可信的。** 只有 Slave 自己持久化在本地的进度文件（`slave-progress.bin`），才是其在灾难恢复中唯一可依赖的“锚点”。

---

## 1. 核心矛盾：我们到底在解决什么问题？

很多初学者认为 Slave 的进度管理流程是这样的：
> 1. Slave 告诉 Master：“我同步到 100 了”。
> 2. Master 记下来。
> 3. Slave 挂了重启，问 Master：“我到哪了？”
> 4. Master 告诉它：“你到 100 了，继续跑吧。”

**这种“托付式”设计在工业级场景下存在三个致命死穴：**

1.  **Master 失忆（单点依赖风险）**：Master 重启后内存数据丢失。
2.  **网络罗生门（状态不一致风险）**：ACK 丢失导致双方认知偏差。
3.  **语义耦合（架构分层风险）**：存储引擎层与复制层逻辑混淆。

---

## 2. 场景推演：三个“死亡场景”

### 🔴 场景一：Master 的失忆症 (Master Restart)
Master 为了高性能，通常将 Slave 的进度维护在 **内存** 中。

* **过程**：Slave 同步到了 LSN 1000，Master 内存记录 `SlaveA = 1000`。
* **故障**：Master 进程崩溃或机器重启，内存数据清空。
* **后果**：
    * Slave 重连。
    * Master 问：“你是谁？你的进度是多少？”
    * 如果 Slave 自己没有记录 `1000`，它只能回答：“我不知道。”
    * Master 只能判定：**“那你从头（LSN 0）开始全量同步吧。”**
* **代价**：触发全量数据传输（Full Resync），引发 GB/TB 级网络风暴，甚至导致 Master 再次被拖垮。

### 🟡 场景二：网络的“测不准原理” (The Network Gap)
即使 Master 不重启，网络的不确定性也会导致“认知偏差”。

* **过程**：
    1.  Slave 拉取了 LSN 1001-2000。
    2.  Slave **落盘成功**。
    3.  Slave 发送 ACK 给 Master —— **此时网线断了/丢包了**。
* **状态不一致**：
    * Slave 真实状态：`Applied = 2000`。
    * Master 认知状态：`Ack = 1000`（因为它没收到 ACK）。
* **故障**：Slave 此时也发生了重启。
* **后果**：
    * **没有本地记录**：Slave 去问 Master。Master 说：“你才到 1000，给你发 1001-2000。” -> **Slave 重复接收数据（带宽浪费 & 数据污染风险）。**
    * **有本地记录**：Slave 读取本地文件：“哦，我已经处理完 2000 了。” -> Slave 直接请求 2001。 -> **完美续传。**

### 🔵 场景三：本地数据损坏 (Corruption & Gap)
这是最极端的场景。

* **过程**：Slave 的磁盘出现坏道，或者人为误删了 WAL 数据文件（Checkpoint 损坏）。
* **救命稻草**：虽然数据丢了，但 `slave-progress.bin` 是一个很小的独立文件，大概率还在。
* **恢复**：Slave 读取进度文件，带着 `LSN=5000` 找 Master。虽然数据对不上，但这个 LSN 仍然是重要的**诊断依据**和**告警阈值**。

---

## 3. 架构分层：为什么不能复用 `checkpoint.bin`？

你可能会问：*“我本地 WAL 引擎已经有一个 `checkpoint.bin` 记录写入位置了，为什么还要一个 `replication-progress.bin`？”*

这是为了**解耦（Decoupling）**。

| 维度 | 本地检查点 (`checkpoint.bin`) | 复制进度 (`slave-progress.bin`) |
| :--- | :--- | :--- |
| **归属层级** | **存储引擎层 (Storage Engine)** | **复制层 (Replication Layer)** |
| **语义** | "我本地磁盘刷盘到了哪里？" | "我从 Master 追随到了哪里？" |
| **更新时机** | 异步刷盘时更新 (Fsync) | 收到 Master 数据并 Apply 后更新 |
| **数值关系** | 通常 `Checkpoint <= Progress` | `Progress` 代表业务层面的最新提交 |
| **变化场景** | 如果 Slave 切换了 Master（Failover），存储层不需要变 | 复制层的进度可能需要重置或校验 |

**工业级设计原则**：
不要让底层的存储引擎感知上层的复制逻辑。如果有一天你想把 WAL 引擎换成 RocksDB，你的复制逻辑（依赖 `slave-progress.bin`）不应受到任何影响。

---

## 4. 实现建议：文件结构与逻辑

这个文件非常重要，但结构很简单。**不要用复杂的 JSON，推荐使用定长二进制或 Mmap。**

### 4.1 物理文件结构 (File Layout)

```text
+----------------+----------------+----------------+----------------+
|  Magic (4B)    |  Version (4B)  |   LSN (8B)     | LastUpdate(8B) |
+----------------+----------------+----------------+----------------+
|   0xCAFEBABE   |       1        | Seg:10, Off:50 |   Timestamp    |
+----------------+----------------+----------------+----------------+